services: 
  frontend:
    build: 
      context: ./frontend
      dockerfile: Dockerfile.dev
    ports:
      - "8080:8080"
    depends_on:
      - backend
    volumes:
      - ./frontend:/app 
      - /app/node_modules
    init: true
  
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    depends_on:
      - redis
    volumes:
      - shared_uploads:/app/uploads
      - shared_transcripts:/app/transcripts
    environment:
      - OLLAMA_HOST=http://ollama:11434
    init: true
  
  worker:
    build: ./celery_worker
    volumes:
      - ./celery_worker:/tasks
      - shared_uploads:/shared/uploads
      - shared_transcripts:/shared/transcripts
    depends_on:
      - redis
      - backend
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - CUDA_VISIBLE_DEVICES=0
    init: true
    command: celery -A tasks worker --concurrency=1 --loglevel=INFO
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
  
  redis:
    image: redis:7
    ports:
      - "6379:6379"
  
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_METAL=1
      - OLLAMA_GPU_TIMEOUT=60s
    volumes:
      - ollama_data:/root/.ollama
    runtime: nvidia
    init: true

  flower:
    image: mher/flower
    ports:
      - "5555:5555"
    environment:
      - CELERY_BROKER_URL=redis://redis:6379/0
    depends_on:
      - redis
    init: true

volumes:
  ollama_data:
  shared_uploads:
  shared_transcripts: